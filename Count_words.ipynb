{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd193b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcaa8487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words:\n",
      "Word\tCount\n",
      "a\t9\n",
      "he\t6\n",
      "the\t6\n",
      "and\t5\n",
      "as\t4\n",
      "was\t4\n",
      "with\t3\n",
      "i\t2\n",
      "of\t2\n",
      "his\t2\n",
      "\n",
      "10 least common words:\n",
      "Word\tCount\n",
      "tables\t1\n",
      "merry\t1\n",
      "word\t1\n",
      "or\t1\n",
      "slap\t1\n",
      "on\t1\n",
      "for\t1\n",
      "more\t1\n",
      "favoured\t1\n",
      "guests\t1\n"
     ]
    }
   ],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Count how many times each unique word occurs in text.\"\"\"\n",
    "    \n",
    "    counts = dict()  # dictionary of { <word>: <count> } pairs to return\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Split text into words, leaving out punctuation\n",
    "    words=re.split(r'[^\\w]',text)\n",
    "    \n",
    "    # Aggregate word counts using a dictionary\n",
    "    for word in words:\n",
    "        if word != \"\":\n",
    "            if word not in counts:\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "\n",
    "def test_run():\n",
    "    with open(\"input.txt\", \"r\") as f:\n",
    "        text = f.read()\n",
    "        counts = count_words(text)\n",
    "        sorted_counts = sorted(counts.items(), key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        print(\"10 most common words:\\nWord\\tCount\")\n",
    "        for word, count in sorted_counts[:10]:\n",
    "            print(\"{}\\t{}\".format(word, count))\n",
    "        \n",
    "        print(\"\\n10 least common words:\\nWord\\tCount\")\n",
    "        for word, count in sorted_counts[-10:]:\n",
    "            print(\"{}\\t{}\".format(word, count))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa08908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting text data into tokens.\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    \"\"\"Split text into sentences.\"\"\"\n",
    "    \n",
    "    # TODO: Split text by sentence delimiters (remove delimiters)\n",
    "    \n",
    "    # TODO: Remove leading and trailing spaces from each sentence\n",
    "    \n",
    "    pass  # TODO: Return a list of sentences (remove blank strings)\n",
    "\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    \"\"\"Split a sentence into words.\"\"\"\n",
    "    \n",
    "    # TODO: Split sent by word delimiters (remove delimiters)\n",
    "    \n",
    "    # TODO: Remove leading and trailing spaces from each word\n",
    "    \n",
    "    pass  # TODO: Return a list of words (remove blank strings)\n",
    "\n",
    "\n",
    "def test_run():\n",
    "    \"\"\"Called on Test Run.\"\"\"\n",
    "\n",
    "    text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war? Is AI a bad thing?\"\n",
    "    print(\"--- Sample text ---\", text, sep=\"\\n\")\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"\\n--- Sentences ---\")\n",
    "    print(sentences)\n",
    "    \n",
    "    print(\"\\n--- Words ---\")\n",
    "    for sent in sentences:\n",
    "        print(sent)\n",
    "        print(word_tokenize(sent))\n",
    "        print()  # blank line for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b84ce7",
   "metadata": {},
   "source": [
    "Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2b3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbf616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c9b11c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnd now for something completely different\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877b5c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043bf13f",
   "metadata": {},
   "source": [
    "Part-of-speech tagging using a predefined grammar like this is a simple, but limited solution. It can be very tedious and error-prone for a large corpus of text, since we have to account for all possible sentence structures and tags!\n",
    "\n",
    "There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf1cc28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnd now for something completely different\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "sentence = word_tokenize(\"And now for something completely different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8ab529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
      "[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NN'), ('life.It', 'NN'), ('exciting', 'VBG'), ('frightening', 'NN'), ('.', '.')]\n",
      "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people.It', 'NN'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "\n",
    "txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n",
    "    \"Sukanya is getting married next year. \" \\\n",
    "    \"Marriage is a big step in one’s life.\" \\\n",
    "    \"It is both exciting and frightening. \" \\\n",
    "    \"But friendship is a sacred bond between people.\" \\\n",
    "    \"It is a special kind of love between us. \" \\\n",
    "    \"Many of you must have tried searching for a friend \"\\\n",
    "    \"but never found the right one.\"\n",
    " \n",
    "# sent_tokenize is one of instances of\n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
    " \n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "     \n",
    "    # Word tokenizers is used to find the words\n",
    "    # and punctuation in a string\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    " \n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    " \n",
    "    #  Using a Tagger. Which is part-of-speech\n",
    "    # tagger or POS-tagger.\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    " \n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5b898",
   "metadata": {},
   "source": [
    "### Name entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c27da5",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16440ad2",
   "metadata": {},
   "source": [
    "### One hot encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548612f8",
   "metadata": {},
   "source": [
    "### t-sne embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d513bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
